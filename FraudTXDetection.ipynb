{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "a7965966-fd3d-4743-ab20-b246beb250c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import avg\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier\n",
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "f315b465-b508-4bf5-8fcd-563198e73f50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# spark session creation\n",
    "def init_spark():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "21c660a6-5622-4a3e-a28c-e222ddc458fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1754155\n"
     ]
    }
   ],
   "source": [
    "# Read csv and print number of data points\n",
    "spark = init_spark()\n",
    "lines = spark.read.csv(\"dataset/FinalTransactions.csv\", header=True)\n",
    "print(lines.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "f5eab630-b559-48cd-9257-8d0f92cc9b53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert features column to number\n",
    "lines = lines.withColumn(\"TX_FRAUD\", col(\"TX_FRAUD\").cast(\"double\"))\n",
    "lines = lines.withColumn('TX_TIME_SECONDS', col('TX_TIME_SECONDS').cast('double'))\n",
    "lines = lines.withColumn('TX_TIME_DAYS', col('TX_TIME_DAYS').cast('double'))\n",
    "lines = lines.withColumn('TX_AMOUNT', col('TX_AMOUNT').cast('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "0af485ba-3b29-4a3f-8c8a-684367119f28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- TRANSACTION_ID: string (nullable = true)\n",
      " |-- TX_DATETIME: string (nullable = true)\n",
      " |-- CUSTOMER_ID: string (nullable = true)\n",
      " |-- TERMINAL_ID: string (nullable = true)\n",
      " |-- TX_AMOUNT: double (nullable = true)\n",
      " |-- TX_TIME_SECONDS: double (nullable = true)\n",
      " |-- TX_TIME_DAYS: double (nullable = true)\n",
      " |-- TX_FRAUD: double (nullable = true)\n",
      " |-- TX_FRAUD_SCENARIO: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Schema definition\n",
    "lines.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "183e18e0-caf6-4254-94de-8dc2e252d0af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "|_c0|TRANSACTION_ID|TX_DATETIME|CUSTOMER_ID|TERMINAL_ID|TX_AMOUNT|TX_TIME_SECONDS|TX_TIME_DAYS|TX_FRAUD|TX_FRAUD_SCENARIO|\n",
      "+---+--------------+-----------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "|  0|             0|          0|          0|          0|        0|              0|           0|       0|                0|\n",
      "+---+--------------+-----------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if any column has null values\n",
    "from pyspark.sql.functions import col, sum, count\n",
    "\n",
    "null_counts = lines.agg(*[\n",
    "    sum(col(c).isNull().cast(\"int\")).alias(c)\n",
    "    for c in lines.columns\n",
    "])\n",
    "\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "84e42e24-62f1-4c13-a565-d8626e5146ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test train split\n",
    "train_data, test_data = lines.randomSplit([0.8, 0.2], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "db19079a-a788-475b-b4ad-7dfc4ac2abd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+------------------+\n",
      "|TX_FRAUD|  count|        percentage|\n",
      "+--------+-------+------------------+\n",
      "|     0.0|1213927| 86.53322355641572|\n",
      "|     1.0| 188918|13.466776443584289|\n",
      "+--------+-------+------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Get class distribution in train_data\n",
    "\n",
    "total_count = train_data.count()\n",
    "class_distrib = train_data.groupby('TX_FRAUD').agg(count(\"*\").alias(\"count\"))\n",
    "class_distrib = class_distrib.withColumn(\"percentage\", (class_distrib[\"count\"] / total_count) * 100)\n",
    "print(class_distrib.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "cf9b1fa1-47f2-4fc5-8c8b-1f773b03bb76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+------------------+\n",
      "|TX_FRAUD_SCENARIO| count|        percentage|\n",
      "+-----------------+------+------------------+\n",
      "|                3|  4631| 1.962545927643038|\n",
      "|                1|222261| 94.19076234590136|\n",
      "|                2|  9077|3.8466917264555938|\n",
      "+-----------------+------+------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Fraud scenario distribution\n",
    "\n",
    "fraud_tx = lines.filter(lines[\"TX_FRAUD\"] != \"0\")\n",
    "total_count = fraud_tx.count()\n",
    "csv_file = fraud_tx.groupby('TX_FRAUD_SCENARIO').agg(count(\"*\").alias(\"count\"))\n",
    "csv_file = csv_file.withColumn(\"percentage\", (csv_file[\"count\"] / total_count) * 100)\n",
    "print(csv_file.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "085608d1-ca42-48c2-9f36-642a7a9e6d93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Upsampling of minority class in train set\n",
    "\n",
    "fraud_count = train_data.filter(col('TX_FRAUD') == 1).count()\n",
    "non_fraud = train_data.filter(col('TX_FRAUD') == 0)\n",
    "fraud = train_data.filter(col('TX_FRAUD') == 1)\n",
    "sampled_non_fraud = non_fraud.sample(False, fraud_count/float(non_fraud.count()))\n",
    "balanced_data = fraud.union(sampled_non_fraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "06c55626-adfd-4662-820f-76ac15efdfeb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------------+\n",
      "|TX_FRAUD| count|       percentage|\n",
      "+--------+------+-----------------+\n",
      "|     1.0|188918|49.91874773748781|\n",
      "|     0.0|189533|50.08125226251219|\n",
      "+--------+------+-----------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Get class distribution in dataset after upsampling\n",
    "\n",
    "total_count = balanced_data.count()\n",
    "class_distrib = balanced_data.groupby('TX_FRAUD').agg(count(\"*\").alias(\"count\"))\n",
    "class_distrib = class_distrib.withColumn(\"percentage\", (class_distrib[\"count\"] / total_count) * 100)\n",
    "print(class_distrib.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "9d9623c8-089d-4dc7-baf4-253b65a4d7f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create feature set in train set\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "# Define the feature columns\n",
    "feature_cols = ['TX_AMOUNT', 'TX_TIME_SECONDS', 'TX_TIME_DAYS']\n",
    "\n",
    "# Create a VectorAssembler to combine the feature columns into a single vector column\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "\n",
    "# Apply the VectorAssembler to the DataFrame to create the vector column\n",
    "vector_df = assembler.transform(balanced_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "ad2adf86-ac25-4af4-a12d-4fc1d159bc2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol='features', outputCol='scaled_features')\n",
    "\n",
    "# Fit the scaler transformer on the vector_df DataFrame\n",
    "scaler_model = scaler.fit(vector_df)\n",
    "\n",
    "# Apply the scaler transformer to the vector_df DataFrame to create the scaled_features column\n",
    "scaled_df = scaler_model.transform(vector_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "e775aa57-28bf-4c7c-9669-dbf8c23b5f32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(_c0='1000', TRANSACTION_ID='1000', TX_DATETIME='2023-01-01 05:31:44', CUSTOMER_ID='1889', TERMINAL_ID='5651', TX_AMOUNT=1474.26, TX_TIME_SECONDS=19904.0, TX_TIME_DAYS=0.0, TX_FRAUD=1.0, TX_FRAUD_SCENARIO='1', features=DenseVector([1474.26, 19904.0, 0.0]), scaled_features=DenseVector([0.6368, 0.0044, 0.0])), Row(_c0='100015', TRANSACTION_ID='100015', TX_DATETIME='2023-01-11 11:05:42', CUSTOMER_ID='3321', TERMINAL_ID='1656', TX_AMOUNT=1070.98, TX_TIME_SECONDS=903942.0, TX_TIME_DAYS=10.0, TX_FRAUD=1.0, TX_FRAUD_SCENARIO='1', features=DenseVector([1070.98, 903942.0, 10.0]), scaled_features=DenseVector([0.4626, 0.1982, 0.1895]))]\n"
     ]
    }
   ],
   "source": [
    "#Print a sample of train set\n",
    "print(scaled_df.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "c0e74aa6-702c-4674-9b96-bd09bb2b7ff0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#define random forest\n",
    "\n",
    "dt = RandomForestClassifier(labelCol='TX_FRAUD', featuresCol='features', maxDepth=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "6f7ce865-c0bf-4603-adbf-a837c11d59de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model training\n",
    "dt_model = dt.fit(scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "cb82f6f5-5906-4a3b-a71f-f3b023fffb32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9793500056452641\n"
     ]
    }
   ],
   "source": [
    "# making prediction & calculating test accuracy\n",
    "\n",
    "# Apply the VectorAssembler to the test_data DataFrame to create the vector column\n",
    "test_vector_df = assembler.transform(test_data)\n",
    "\n",
    "# Apply the scaler transformer to the test_vector_df DataFrame to create the scaled_features column\n",
    "test_scaled_df = scaler_model.transform(test_vector_df)\n",
    "\n",
    "predictions = dt_model.transform(test_vector_df)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=label_col)\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(f\"AUC: {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "f6ee8c5e-c512-4833-8db8-2f8ad7292890",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9946083675811852\n",
      "Recall: 0.9945745922404714\n"
     ]
    }
   ],
   "source": [
    "#calculating Precision & recall\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=label_col, metricName=\"weightedPrecision\")\n",
    "precision = evaluator.evaluate(predictions)\n",
    "print(f\"Precision: {precision}\")\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=label_col, metricName=\"weightedRecall\")\n",
    "recall = evaluator.evaluate(predictions)\n",
    "print(f\"Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "d437d35f-73f3-42df-9d6b-ece5163a7fc4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.995\n"
     ]
    }
   ],
   "source": [
    "#Calculating F1 score\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=label_col, predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "# Compute the F1 score\n",
    "f1_score = evaluator.evaluate(predictions)\n",
    "print(f\"F1 Score: {f1_score:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
